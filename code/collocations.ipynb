{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Notebook for Collocation Analysis\n",
    "\n",
    "Collocation analysis using ```nltk```."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "import os,re,string,json,math\n",
    "from functions import *\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "plotting.style_()\n",
    "\n",
    "tf = pd.read_csv(base_path + '/resources/keywords-corona-translation.csv')\n",
    "tf_corona = {language:dict(zip(tf[tf['language'] == language]['word'],tf[tf['language'] == language]['translation'])) for language in list(set(tf['language']))}\n",
    "\n",
    "tf = pd.read_csv(base_path + '/resources/keywords-expertise-translation.csv')\n",
    "tf_science = {language:dict(zip(tf[tf['language'] == language]['word'],tf[tf['language'] == language]['translation'])) for language in list(set(tf['language']))}\n",
    "tf_science = {k:{x:i for x,i in v.items() if x not in [\"policy\",\"program\",\"measures\"]} for k,v in tf_science.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad hoc function for importing data and preprocessing.\n",
    "\n",
    "def clean_subset(iso,language,start,end):\n",
    "\n",
    "    \"\"\" Function for importing subset of the data\n",
    "    iso (str): ISO code for language (\"gb\", \"nl\" etc.)\n",
    "    language (str): name of language for calling nltk stopwords (\"english\", \"dutch\" etc.)\n",
    "    start (str): start month (\"2020-01\")\n",
    "    end (str): end month (\"2020-02\")\n",
    "    \"\"\"\n",
    "    \n",
    "    df = data_loader.load_month(iso,start,end)\n",
    "    df = df = df[df['text'].notna()]\n",
    "    if language in os.listdir('/home/ruben/nltk_data/corpora/stopwords/'):\n",
    "        stopwords_ = nltk.corpus.stopwords.words(language)\n",
    "    else:\n",
    "        stopwords_ = []\n",
    "    df['text'] = [[w for w in str(t).split(' ') if \"_\" in w and len(w.split('_')) != 1] for t in df['posner']]\n",
    "    df['text'] = [\" \".join([w.split('_')[0] for w in text if w.split('_')[1] in ['NOUN','VERB','ADJ']]) for text in df['text']]\n",
    "    df['text'] = [utils.preprocess(str(x),stopwords_) for x in tqdm(df['text'])]\n",
    "    df = df.drop(['lemmatized','title'],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_collocates(bgfinder, vocabulary, seed_term, topn):\n",
    "\n",
    "    \"\"\" Function for finding top collocates of seed term\n",
    "    bgfinder (<BigramCollocationFinder> object): finder object\n",
    "    vocabulary (list): list of terms to consider\n",
    "    seed_term (str): seed term\n",
    "    top_n (int): number of collocates to return\n",
    "    \"\"\"\n",
    "\n",
    "    list_scores = {w:bgfinder.score_ngram(bigram_measures.likelihood_ratio,seed_term,w) for w in vocabulary}\n",
    "    list_scores = {k:v for k,v in list_scores.items() if v != None}\n",
    "    return list(dict(sorted(list_scores.items(), key = itemgetter(1), reverse = True)[:topn]).keys())\n",
    "\n",
    "def get_network(bgfinder, vocabulary, seed_term, topn):\n",
    "\n",
    "    \"\"\" Function for finding top collocates of seed term\n",
    "    bgfinder (<BigramCollocationFinder> object): finder object\n",
    "    vocabulary (list): list of terms to consider\n",
    "    seed_term (str): seed term\n",
    "    top_n (int): number of collocates to return\n",
    "    \"\"\"\n",
    "\n",
    "    d = []\n",
    "    for w1 in find_top_collocates(bgfinder, vocabulary, seed_term,topn):\n",
    "        d.append([seed_term,w1])\n",
    "        for w2 in find_top_collocates(bgfinder, vocabulary, w1,topn):\n",
    "            d.append([w1,w2])\n",
    "            for w3 in find_top_collocates(bgfinder, vocabulary, w2,topn):\n",
    "                d.append([w2,w3])\n",
    "    return pd.DataFrame(d,columns=['source','target'])\n",
    "\n",
    "def collocation_month(iso,df,month,seed_term,window_size=15,topn=6,plot=True,degree_limit=0):\n",
    "    \n",
    "    \"\"\" Function for drawing collocation network for one month\n",
    "    iso (str): ISO code\n",
    "    df (<DataFrame> object): dataframe with text and metadata\n",
    "    month (str): month\n",
    "    seed_term (str): seed term\n",
    "    window_size (int): size of window to use for finding collocates \n",
    "    topn (int): top collocates to consider\n",
    "    plot (boolean): whether to plot network in notebook\n",
    "    degree_limt (int): limit network to nodes with a min. degree\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[(df['id'].str.contains(month))]\n",
    "    text = \" \".join(df['text']).split(' ')\n",
    "    finder = BigramCollocationFinder.from_words(text,window_size=window_size)\n",
    "    d = get_network(seed_term,finder,set(text),topn)\n",
    "    g = nx.from_pandas_edgelist(d, source='source', target='target',create_using=nx.DiGraph()) \n",
    "    dgrs = dict(g.degree)\n",
    "\n",
    "    if degree_limit != 0:\n",
    "        d = d[d['target'].isin([k for k,v in dgrs.items() if v >= degree_limit])]\n",
    "        g = nx.from_pandas_edgelist(d, source='source', target='target',create_using=nx.DiGraph()) \n",
    "        dgrs = dict(g.degree)\n",
    "    \n",
    "    if plot == True:\n",
    "        plt.figure(figsize=(25,15))\n",
    "        layout = nx.spring_layout(g,k=1.15)\n",
    "\n",
    "        nx.draw_networkx_nodes(g,layout,node_size=2,alpha=0)\n",
    "        nx.draw_networkx_edges(g, layout, width=1.5, alpha=.75, edge_color=\"#cccccc\",arrows=True,arrowstyle=\"-|>\",arrowsize=50)\n",
    "        \n",
    "        for node, (x, y) in layout.items():\n",
    "            plt.text(x, y, node, fontsize=math.log(dgrs[node] * 5) * 6, ha='center', va='center',color = \"red\" if node == seed_term else \"black\",bbox=dict(facecolor='red', alpha=0.1))\n",
    "        plt.title(f\"Collocation Network for seed term {seed_term.upper()} in {month} (Corpus: {iso.upper()})\",fontsize=24)\n",
    "        plt.savefig(f'/results/plots/collocation-networks/collocation-network-{language}-{month}-{seed_term}-topn{topn}-ws{window_size}.png',dpi=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot network for different languages and write to file\n",
    "\n",
    "for iso,language in [('nl','dutch'),('it','italian'),('gb','english')]:\n",
    "    df = clean_subset(iso,language)\n",
    "    for month in [\"2020-03\",\"2020-04\",\"2020-05\",\"2020-06\"]:\n",
    "        \n",
    "        for term in ['expert','science']:\n",
    "            collocation_month(iso,df,month=month,seed_term=tf_science[iso][term],window_size=5,topn=5,plot=True,degree_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get top collocates for 'expert' per month per coalition-opposition\n",
    "\n",
    "for iso,language in [('nl','dutch'),('it','italian'),('gb','english')]:\n",
    "    df = clean_subset(iso,language)\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for month in [\"2020-03\",\"2020-04\",\"2020-05\",\"2020-06\"]:\n",
    "        tdf = df[(df['id'].str.contains(month))]\n",
    "        for group in ['coalition','opposition']:\n",
    "            print(month,group)\n",
    "            tdfg = tdf[tdf['party_status'] == group]\n",
    "            text = list(tdfg['text'])\n",
    "            text = \" \".join(text).split(' ')\n",
    "            len_ = len([w for w in text if w  == tf_science[iso]['expert']])\n",
    "            if len_ < 5:\n",
    "                continue\n",
    "            finder = BigramCollocationFinder.from_words(text,window_size=5)\n",
    "            result[month + \"_\" + group + f\"(n={len_})\"] = find_top_collocates(finder, set(text), tf_science[iso]['expert'], 15)\n",
    "    result.to_csv(f'results/tables/coalopp-topcollocates-expert-{iso}-ws5-alldata.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nl\n",
      "100%|██████████| 27378/27378 [00:03<00:00, 7793.49it/s]\n",
      "size after subsetting: 27378\n",
      "100%|██████████| 50001/50001 [00:05<00:00, 9132.03it/s]\n",
      "size after subsetting: 50001\n",
      "it\n",
      "100%|██████████| 2781/2781 [00:04<00:00, 621.67it/s]\n",
      "size after subsetting: 2781\n",
      "100%|██████████| 2564/2564 [00:03<00:00, 768.80it/s]\n",
      "size after subsetting: 2564\n",
      "gb\n",
      "100%|██████████| 37280/37280 [00:11<00:00, 3111.27it/s]\n",
      "size after subsetting: 37280\n",
      "100%|██████████| 27476/27476 [00:07<00:00, 3554.77it/s]\n",
      "size after subsetting: 27476\n",
      "pl\n",
      "100%|██████████| 18120/18120 [00:01<00:00, 10664.92it/s]\n",
      "size after subsetting: 18120\n",
      "100%|██████████| 17575/17575 [00:01<00:00, 9129.05it/s]\n",
      "size after subsetting: 17575\n"
     ]
    }
   ],
   "source": [
    "## Get top collocates for 'expert' per period (covid/reference)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for iso,language in [('nl','dutch'),('it','italian'),('gb','english'),('pl','polish')]:\n",
    "    print(iso)\n",
    "    keyword = tf_science[iso]['expert']\n",
    "    df_covid = clean_subset(iso,language,\"2020-03\",\"2020-08\")\n",
    "    df_reference = clean_subset(iso,language,\"2019-08\",\"2020-02\")\n",
    "\n",
    "    text_covid = \" \".join(df_covid['text']).split(' ')\n",
    "    finder_covid = BigramCollocationFinder.from_words(text_covid,window_size=10)\n",
    "    finder_covid.apply_ngram_filter(lambda *w: keyword not in w)\n",
    "\n",
    "    text_ref = \" \".join(df_reference['text']).split(' ')\n",
    "    finder_ref = BigramCollocationFinder.from_words(text_ref,window_size=10)\n",
    "    finder_ref.apply_ngram_filter(lambda *w: keyword not in w)\n",
    "    results['covid' + \"_\" + iso + f\"({dict(Counter(text_covid))[tf_science[iso]['expert']]})\"] = list(set([[x for x in i if x != keyword][0] for i in finder_covid.nbest(bigram_measures.likelihood_ratio, 50) if i != (keyword,keyword)]))[:25]\n",
    "    results['reference' + \"_\" + iso + f\"({dict(Counter(text_ref))[tf_science[iso]['expert']]})\"] = list(set([[x for x in i if x != keyword][0] for i in finder_ref.nbest(bigram_measures.likelihood_ratio, 50) if i != (keyword,keyword)]))[:25]\n",
    "\n",
    "    ref_colname = 'reference' + \"_\" + iso + f\"({dict(Counter(text_ref))[tf_science[iso]['expert']]})\"\n",
    "    cov_colname = 'covid' + \"_\" + iso + f\"({dict(Counter(text_covid))[tf_science[iso]['expert']]})\"\n",
    "    # unique_covid = set(results[cov_colname]) - set(results[ref_colname])\n",
    "    # results[cov_colname] = [f\"{x} (U)\" if x in unique_covid else x for x in results[cov_colname]]\n",
    "\n",
    "results.to_csv('topcollocates-expert-covidref.csv',index=False)"
   ]
  }
 ]
}